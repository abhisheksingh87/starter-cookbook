[{"uri":"/recipes-reactive/spring-webflux-webclient/","title":"10.Spring WebFlux WebClient","tags":["reactive","spring","reactor","spring webflux"],"description":"","content":"CONTEXT This recipe provides the context of Why Reactive Programming and Use cases\n"},{"uri":"/recipes-reactive/why-reactive-and-when-to-use/","title":"5. Why Reactive","tags":["reactive","spring","reactor","spring webflux"],"description":"","content":"CONTEXT This recipe provides the context of Why Reactive Programming and Use cases\nWhat is Reactive Programming Reactive Programming is a new paradigm in which you use declarative code (in a manner that is similar to functional programming) in order to build asynchronous processing pipelines. It is an event-based model where data is pushed to the consumer, as it becomes available: we deal with asynchronous sequences of events\nReactive code is assembled around asynchronous functional chains, where inputs are streamed (propagated) through these chains from the producer to the subscribers.\nWhy Reactive Before we plunge into technical aspects of Reactive Systems and architectures, we should know \u0026ldquo;Why build Reactive Systems\u0026rdquo;? Let\u0026rsquo;s say that our system should:\n  Responsive to interactions with its users.\n  Handle failure and remain available during outages.\n  Strive under varying load conditions.\n  Be able to send, receive, and route messages in varying network conditions.\nThese answers actually convey the core reactive traits as defined in the reactive-manifesto   So how should an application be responsive? One of the first ways to achieve responsiveness is through elasticity. This describes the ability to stay responsive under a varying workload, meaning the throughput of the system should increase automatically when more users start using it and it should decrease automatically when the demand goes down. From the application perspective, this feature enables system responsiveness because any point in time the system can be expanded without affecting average latency. The acceptance criteria for the system are the ability to stay responsive under failures, or, in other words, to be resilient. This may be achieved by applying isolation between functional components of the system, thereby isolating all internal failures and enabling independence\nAnother point to emphasize is that elasticity and resilience are tightly coupled, and we achieve a truly responsive system only by enabling both. With scalability, we can have multiple replicas of the component so that, if one fails, we can detect this, minimize its impact on the rest of the system, and switch to another replica.\n Elasticity Failure Isolation Component  Message-Driven Communication One of the widely asked question is how to connect components in the distributed system and preserve decoupling, isolation, and scalability at the same time. Let\u0026rsquo;s deep dive into below code\n@RequestMapping(\u0026#34;/resource\u0026#34;) // (1)  public Object processRequest() { RestTemplate template = new RestTemplate(); // (2)  ExamplesCollection result = template.getForObject( // (3)  \u0026#34;http://abc.com/api/resource2\u0026#34;, ExamplesCollection.class ); processResultFurther(result); // (4)  } In the preceding example, we have defined the request handler which will be invoked on users' requests. In turn, each invocation of the handler produces an additional HTTP call to an external service and then subsequently executes another processing stage. Despite the fact that the preceding code may look familiar and transparent in terms of logic, it has some flaws. To understand what is wrong in this example, let\u0026rsquo;s take an overview of the following request\u0026rsquo;s timeline: only a small part of the processing time is allocated for effective CPU usage whereas the rest of the time thread is being blocked by the I/O and cannot be used for handling other requests. Java world, we have thread pools, which may allocate additional threads to increase parallel processing. However, under a high load, such a technique may be extremely inefficient to process the new I/O task simultaneously. So, So to achieve better resource utilization in I/O cases, we should use an asynchronous and non-blocking interaction model. In real life, this kind of communication is messaging. Project Reactor Project Reactor is a fully non-blocking foundation with back-pressure support included. It’s the foundation of the reactive stack in the Spring ecosystem and is featured in projects such as Spring WebFlux, Spring Data, and Spring Cloud Gateway.\nReactive Microservices with Spring Boot The Spring portfolio provides two parallel stacks. One is based on a Servlet API with Spring MVC and Spring Data constructs. The other is a fully reactive stack that takes advantage of Spring WebFlux and Spring Data’s reactive repositories. In both cases, Spring Security has you covered with native support for both stacks. MVC vs WebFlux     MVC WebFlux     Scaling Multithreading Event Loop   Default Server Tomcat Netty   Responses List, Object, ResponseEntity Flux, Mono   Routing Annotational@Controller, @RequestMapping, @Controller, @RequestMapping, @GetMapping, etc. Annotational@Controller, @RequestMapping, @GetMapping, etc.Functional Router+Handler    Performance The performance statistics below is between spring boot 2 webflux vs spring boot 1. The applications used along with scripts are available here\nPerformance: 300 concurrent users     Synchronous Asynchronous     Minimum Response Time 302ms 301ms   Maximum Response Time Server 317ms 323ms   Mean Response Time 306ms 305ms   Mean Requests/Second 102.273 103.448    Performance: 3000 concurrent users     Synchronous Asynchronous     Minimum Response Time 302ms 301ms   Maximum Response Time Server 3734ms 769ms   Mean Response Time 2506ms 313ms   Mean Requests/Second 596.026 1011.236    When: Use Cases When you may not want to Use Reactive  An app requires a significant amount of rework from Traditional MVC. Your app is a traditional CRUD-style app. You have a relational DB, but no reactive driver or abstraction . App has low concurrency (\u0026lt; 1,000 concurrent users). Using an older version of Servlet (3.1+ is required). Your data sources are blocking on I/O and do not scale. You depend on ORM features like caching, lazy loading, write-behind.  When you may want to use Reactive   Relational Database with reactive driver or R2DBC abstraction (MS SQL, Postgres).\n  Non-relational reactive DB (Mongo, Cassandra, Redis).\n  Application with more than 1000 concurrent users.\n Web/Mobile apps. Remote apps requiring data refresh(ex- backpressure).    A large number of transaction-processing services.\n  Notification services.\n A banking example of application around Reactive principles could be online car shopping and financing. Customers can browse million cars from over thousand\u0026rsquo;s of dealers and pre-qualify for financing in seconds, without impacting credit scores.\n   "},{"uri":"/recipes-reactive/spring-webflux-rx-api/","title":"6. Spring WebFlux lite reactive api","tags":["reactive","spring","reactor","spring webflux"],"description":"","content":"CONTEXT This recipe provides the context of Why Reactive Programming and Use cases\n"},{"uri":"/recipes-reactive/spring-webflux-annotated-controllers/","title":"7. Spring WebFlux Annotated Controllers","tags":["reactive","spring","reactor","spring webflux","annotated controllers"],"description":"","content":"CONTEXT This recipe provides the context of Why Reactive Programming and Use cases\n"},{"uri":"/recipes-reactive/spring-webflux-functional-endpoints/","title":"8. Spring WebFlux Functional Endpoints","tags":["reactive","spring","reactor","spring webflux"],"description":"","content":""},{"uri":"/recipes-reactive/spring-webflux-exception-handling/","title":"9. Spring WebFlux Exception Handling","tags":["reactive","spring","reactor","spring webflux"],"description":"","content":""},{"uri":"/recipes-non-reactive/configure-actuator/","title":"2. Configure Actuators","tags":["application development","Spring","Spring Boot","actuator","endpoint","health","health check"],"description":"","content":"CONTEXT The Spring Boot Actuator provides production-ready features for our Spring Boot application. Actuators can be useful for providing diagnostics, information, metrics, and controls on your Spring Boot application running in dev, production, on-prem, or the cloud. It is highly recommended including and enabling Actuators if you are writing a Spring Boot application.\nSOLUTION Actuator Basics In essence, Actuator brings production-ready features to our application. It monitors app, gather metrics, determines the state of Database and much more. The main benefit of this library is that we can get production-grade tools without having to actually implement these features ourselves. Actuator is mainly used to expose operational information about the application: health, metrics, info, dump, env, etc. It uses Http endpoints or JMX beans to expose information.\nSetup:   Add below dependency to build.gradle.\nimplementation: \u0026#39;org.springframework.boot:spring-boot-starter-actuator\u0026#39;   Configuring Actuators  Spring Boot 2.x onwards Actuator comes with most endpoints disabled. The only two endpoints available by default are /health and /info. By Default, All Actuator endpoints are now placed under /actuator path  Predefined Endpoints  Please find below details of actuator endpoints.    Endpoint Details     /health Shows application health information.   /info Shows general information. It might be custom data, build information or details about the latest commit   /env Shows current environment properties.   /configprops Allows to fetch all @ConfigurationProperties beans.   /heapdump Builds and Returns a heap dump from the JVM used by the application.   /metrics Shows ‘metrics’ information for the current application..   /threaddump Performs a thread dump.   /conditions Shows the conditions that were evaluated on configuration and auto-configuration classes and the reason why they did or did not match.   /beans Displays a complete list of all the Spring beans in the application.   /auditevents Exposes audit events information for the current application. Requires an AuditEventRepository bean.      Hypermedia for Actuator Endpoints   Spring boot adds a discovery endpoint that returns links to all available actuator endpoints. This facilitates discovering actuator endpoints and their corresponding URLs.\n  By default, this discovery endpoint is accessible through the /actuator endpoint.\n  Enable Http Endpoints All web actuator endpoints can be enabled by adding property in application.yml or application.properties:\nmanagement:endpoints:web:exposure:include:*If you want to enable specific web endpoints you can enable them in application.yml or application.properties:\nmanagement:endpoints:web:exposure:include:health,info,env,metricsBy default, /health endpoint only shows below:\n{ \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34; } you can enable show-details management health property in application.yml to provide detailed status of the application:\nmanagement:endpoint:health:show-details:alwaysyou can also disable all web endpoints in application.yml:\nmanagement:endpoints:enabled-by-default:falseEnable All JMX Endpoints Java Management Extensions (JMX) provide a standard mechanism to monitor and manage applications. By default, Spring Boot exposes management endpoints as JMX MBeans under the org.springframework.boot domain. you can customize the JMX domain under which endpoints are exposed using below\nmanagement:endpoints:jmx:domain:com.wellsfargo.cto.eai All JMX actuator endpoints can be enabled by adding property in application.yml or application.properties:\nmanagement:endpoints:jmx:exposure:include:*If you want to enable specific JMX endpoints you can enable them in application.yml or application.properties:\nmanagement:endpoints:jmx:exposure:include:health,info,env,metricsAll JMX endpoints can be disabled by:\nendpoints:default:jmx:enabled:falseEnable Actuators based on Profiles:   Spring profiles provide a way to segregate parts of your application configuration and make it only available in certain environments. We can utilize spring profiles to enable to disable endpoints based on environments.\n  By using spring.profiles.active=local Spring will look for a file application-local.yml and will try to load configurations. We can specify the management endpoints that we need for development/production by following this approach.\n  "},{"uri":"/recipes-reactive/configure-actuator/","title":"2. Configure Actuators","tags":["application development","Spring","Spring Boot","actuator","endpoint","health","health check"],"description":"","content":"CONTEXT The Spring Boot Actuator provides production-ready features for our Spring Boot application. Actuators can be useful for providing diagnostics, information, metrics, and controls on your Spring Boot application running in dev, production, on-prem, or the cloud. It is highly recommended including and enabling Actuators if you are writing a Spring Boot application.\nSOLUTION Actuator Basics Actuators In essence, Actuator brings production-ready features to our application. It monitors app, gather metrics, determines the state of Database and much more. The main benefit of this library is that we can get production-grade tools without having to actually implement these features ourselves. Actuator is mainly used to expose operational information about the application: health, metrics, info, dump, env, etc. It uses Http endpoints or JMX beans to expose information.\nSetup:   Add below dependency to build.gradle.\nimplementation: \u0026#39;org.springframework.boot:spring-boot-starter-actuator\u0026#39;   Configuring Actuators  Spring Boot 2.x onwards Actuator comes with most endpoints disabled. The only two endpoints available by default are /health and /info. By Default, All Actuator endpoints are now placed under /actuator path  Predefined Endpoints  Please find below details of actuator endpoints.    Endpoint Details     /health Shows application health information.   /info Shows general information. It might be custom data, build information or details about the latest commit   /env Shows current environment properties.   /configprops Allows to fetch all @ConfigurationProperties beans.   /heapdump Builds and Returns a heap dump from the JVM used by the application.   /metrics Shows ‘metrics’ information for the current application..   /threaddump Performs a thread dump.   /conditions Shows the conditions that were evaluated on configuration and auto-configuration classes and the reason why they did or did not match.   /beans Displays a complete list of all the Spring beans in the application.   /auditevents Exposes audit events information for the current application. Requires an AuditEventRepository bean.      Hypermedia for Actuator Endpoints   Spring boot adds a discovery endpoint that returns links to all available actuator endpoints. This facilitates discovering actuator endpoints and their corresponding URLs.\n  By default, this discovery endpoint is accessible through the /actuator endpoint.\n  Enable Http Endpoints All web actuator endpoints can be enabled by adding property in application.yml or application.properties:\nmanagement:endpoints:web:exposure:include:*If you want to enable specific web endpoints you can enable them in application.yml or application.properties:\nmanagement:endpoints:web:exposure:include:health,info,env,metricsBy default, /health endpoint only shows below:\n{ \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34; } you can enable show-details management health property in application.yml to provide detailed status of the application:\nmanagement:endpoint:health:show-details:alwaysyou can also disable all web endpoints in application.yml:\nmanagement:endpoints:enabled-by-default:falseEnable All JMX Endpoints Java Management Extensions (JMX) provide a standard mechanism to monitor and manage applications. By default, Spring Boot exposes management endpoints as JMX MBeans under the org.springframework.boot domain. you can customize the JMX domain under which endpoints are exposed using below\nmanagement:endpoints:jmx:domain:com.wellsfargo.cto.eai All JMX actuator endpoints can be enabled by adding property in application.yml or application.properties:\nmanagement:endpoints:jmx:exposure:include:*If you want to enable specific JMX endpoints you can enable them in application.yml or application.properties:\nmanagement:endpoints:jmx:exposure:include:health,info,env,metricsAll JMX endpoints can be disabled by:\nendpoints:default:jmx:enabled:falseEnable Actuators based on Profiles:   Spring profiles provide a way to segregate parts of your application configuration and make it only available in certain environments. We can utilize spring profiles to enable to disable endpoints based on environments.\n  By using spring.profiles.active=local Spring will look for a file application-local.yml and will try to load configurations. We can specify the management endpoints that we need for development/production by following this approach.\n  "},{"uri":"/recipes-reactive/configure-mongodb/","title":"3. Configure MongoDB","tags":["persistence","mongodb","spring boot"],"description":"","content":"CONTEXT This is the second recipe in the series, for developing a modern cloud ready microservice using the greenfield-app-starter.\nThis recipe deals with configuring persistence in the microservice.\nPrerequisite  STEP 1: CREATE APPLICATION USING THE STARTER is completed.  SOLUTION   Determine and record the following mongodb database connection details\n   Property Details     uri mongodb://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database name\u0026gt;   database-name database name   username username (plain text)   password password (encrypted)      Navigate to the \u0026lt;microservice\u0026gt; directory\n  Update the database connection and connection pool requirements in src/main/resources/application.yml\napplication:name:description:id:\u0026lt;wells fargo distributed-id\u0026gt;persistence:mongodb:host:port:database-name:username:password:  Validation   Open a command window in the \u0026lt;microservice-name\u0026gt; directory\n  Validate the new microservice can be built locally: gradlew bootJar\n  Validate the new microservice runs locally: gradlew bootRun\n  Verify microservice health and info in the browser\n  http://localhost:8080/actuator/info\n  http://localhost:8080/actuator/health\nthe datasource details and uptime status should be displayed in the browser.\n  http://localhost:8080/actuator/beans\nsearch for mongo in the browser and you should see below\n\u0026#34;mongo\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;4.2.0\u0026#34; } },     NOTES you can also use properties provided by Spring to configure mongodb\nspring:mongodb:host:port:uri:database:username:password:"},{"uri":"/recipes-reactive/accessing-data-mongodb-reactive/","title":"4. Access Data in MongoDB","tags":["persistence","data-repository","mongodb","spring boot"],"description":"","content":"CONTEXT This guide walks you through the process of how to configure and implement database operations using Reactive Programming through Spring data Reactive Repositories and Template with MongoDB.\nSOLUTION Define Entity MongoDB is a NoSQL document store. In this example, you store Customer objects.\n@Builder @AllArgsConstructor @Getter @Setter @ToString public class Account { @Id private String id; private Long accountNumber; private Long routingNumber; private String accountOwner; }   Account class has four attributes: id, accountNumber, routingNumber and accountOwner. The id is mostly for internal use by MongoDB. You also have a single constructor to populate the entities when creating a new instance.\n  id fits the standard name for a MongoDB ID, so it does not require any special annotation to tag it for Spring Data MongoDB.\n  The other three properties, accountNumber, routingNumber and accountOwner are left unannotated. It is assumed that they are mapped to fields that share the same name as the properties themselves.\n  The convenient @ToString() method prints out the details about a customer.\n NOTE\n MongoDB stores data in collections. Spring Data MongoDB maps the Customer class into a collection called customer. If you want to change the name of the collection, you can use Spring Data MongoDB\u0026rsquo;s @Document annotation on the class.   Create Queries ReactiveMongoRepository Spring Data MongoDB focuses on storing data in MongoDB. ReactiveMongoRepository interface inherits from ReactiveCrudRepository and adds new query methods:\nimport org.springframework.data.mongodb.repository.ReactiveMongoRepository; public interface AccountRepository extends ReactiveMongoRepository\u0026lt;Account, String\u0026gt; { Mono\u0026lt;Account\u0026gt; findByAccountOwner(String accountOwner); @Query(\u0026#34;{ \u0026#39;accountNumber\u0026#39;: ?0, \u0026#39;routingNumber\u0026#39;: ?1}\u0026#34;) Mono\u0026lt;Account\u0026gt; findByAccountNumberAndRoutingNumber(Long accountNumber, Long routingNumber); } Using the ReactiveMongoRepository, we can query by example.\nReactiveMongoTemplate Besides the repositories approach, there is also ReactiveMongoTemplate:\n@Service @AllArgsConstructor public class AccountTemplateOperations { private final ReactiveMongoTemplate template; public Mono\u0026lt;Account\u0026gt; findById(String id) { return template.findById(id, Account.class); } public Flux\u0026lt;Account\u0026gt; findAll() { return template.findAll(Account.class); } public Mono\u0026lt;Account\u0026gt; save(Mono\u0026lt;Account\u0026gt; account) { return template.save(account); } public Flux\u0026lt;Account\u0026gt; findByFirstNameAndLastName(String firstName, String lastName) { Query query = new Query(); query.addCriteria(Criteria.where(\u0026#34;firstName\u0026#34;).is(firstName).and(\u0026#34;lastName\u0026#34;).is(lastName)); return template.find(query, Account.class); } } Testing   Using Junit5 and Spring Test Framework we can write Integration Tests for MongoDB\n  Project reactor provides library io.projectreactor:reactor-test which is used to test reactive streams. One of the key elements in reactor test library is: StepVerifier.\n  StepVerifier provides a declarative way of creating verifiable steps for async publisher sequence by expressing expectations about the set of events that will eventually happen upon subscription. Example below\n  @ExtendWith(SpringExtension.class) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT, classes = GreenfieldReactiveApplication.class) public class AccountRepositoryTest { @Autowired private AccountMongoRepository repository; @Test public void testFindById() { //given  Account account = repository.save(new Account(null, 918345L, 234518L, \u0026#34;alex\u0026#34;)) .block(); //when  Mono\u0026lt;Account\u0026gt; createdAccount = repository.findById(account.getId()); //then  StepVerifier .create(createdAccount) .assertNext(acc -\u0026gt; { assertThat(\u0026#34;alex\u0026#34;).isEqualTo(acc.getAccountOwner()); assertThat(234518L).isEqualTo(acc.getRoutingNumber()); assertThat(acc.getId()).isNotNull(); }) .expectComplete() .verify(); } @Test public void testSave() { Mono\u0026lt;Account\u0026gt; accountMono = repository.save(new Account(null, 918345L, 234518L, \u0026#34;alex\u0026#34;)); StepVerifier .create(accountMono) .assertNext(account -\u0026gt; assertThat(account.getId()).isNotNull()) .expectComplete() .verify(); } @Test public void testFindByAccountOwner() { //given  Account account = repository.save(new Account(null, 918345L, 234518L, \u0026#34;ron\u0026#34;)) .block(); //when  Mono\u0026lt;Account\u0026gt; createdAccount = repository.findByAccountOwner(account.getAccountOwner()); //then  StepVerifier .create(createdAccount) .assertNext(acc -\u0026gt; { assertThat(\u0026#34;ron\u0026#34;).isEqualTo(acc.getAccountOwner()); assertThat(234518L).isEqualTo(acc.getRoutingNumber()); assertThat(acc.getId()).isNotNull(); }) .expectComplete() .verify(); } @Test public void testFindByAccountNumberAndRoutingNumber() { //given  Account account = repository.save(new Account(null, 918345L, 235189L, \u0026#34;john\u0026#34;)) .block(); //when  Mono\u0026lt;Account\u0026gt; createdAccount = repository.findByAccountNumberAndRoutingNumber(account.getAccountNumber(), account.getRoutingNumber()); //then  StepVerifier .create(createdAccount) .assertNext(acc -\u0026gt; { assertThat(\u0026#34;john\u0026#34;).isEqualTo(acc.getAccountOwner()); assertThat(235189L).isEqualTo(acc.getRoutingNumber()); assertThat(acc.getId()).isNotNull(); }) .expectComplete() .verify(); } @Test public void testFindAll() { //given  repository.save(new Account(null, 918345L, 234518L, \u0026#34;mike\u0026#34;)) .block(); ExampleMatcher matcher = ExampleMatcher.matching().withMatcher(\u0026#34;accountOwner\u0026#34;, startsWith()); Example\u0026lt;Account\u0026gt; example = Example.of(new Account(null, 918345L, 234518L, \u0026#34;mike\u0026#34;), matcher); //when  Flux\u0026lt;Account\u0026gt; accountFlux = repository.findAll(example); //then  StepVerifier .create(accountFlux) .assertNext(account -\u0026gt; assertThat(\u0026#34;mike\u0026#34;).isEqualTo(account.getAccountOwner())) .expectComplete() .verify(); } }  NOTE "},{"uri":"/recipes-non-reactive/accessing-data-mongodb/","title":"5. Access Data in MongoDB","tags":["persistence","data-repository","mongodb","spring boot"],"description":"","content":"CONTEXT This guide walks you through the process of using Spring Data MongoDB to build an application that stores data in and retrieves it from MongoDB, a document-based database.\nSOLUTION Define Entity MongoDB is a NoSQL document store. In this example, you store Customer objects.\n@Builder @AllArgsConstructor @Getter @Setter @ToString public class Account { @Id private String id; private Long accountNumber; private Long routingNumber; private String accountOwner; }   Account class has four attributes: id, accountNumber, routingNumber and accountOwner. The id is mostly for internal use by MongoDB. You also have a single constructor to populate the entities when creating a new instance.\n  id fits the standard name for a MongoDB ID, so it does not require any special annotation to tag it for Spring Data MongoDB.\n  The other three properties, accountNumber, routingNumber and accountOwner are left unannotated. It is assumed that they are mapped to fields that share the same name as the properties themselves.\n  The convenient @ToString() method prints out the details about a customer.\n NOTE\n MongoDB stores data in collections. Spring Data MongoDB maps the Customer class into a collection called customer. If you want to change the name of the collection, you can use Spring Data MongoDB\u0026rsquo;s @Document annotation on the class.   Create Queries Spring Data MongoDB focuses on storing data in MongoDB. It also inherits functionality from the Spring Data Commons project, such as the ability to derive queries.\nimport org.springframework.data.mongodb.repository.MongoRepository; public interface AccountRepository extends MongoRepository\u0026lt;Account, String\u0026gt; { Account findByAccountOwner(String accountOwner); @Query(\u0026#34;{ \u0026#39;accountNumber\u0026#39;: ?0, \u0026#39;routingNumber\u0026#39;: ?1}\u0026#34;) Account findByAccountNumberAndRoutingNumber(Long accountNumber, Long routingNumber); }   AccountRepository extends the MongoRepository interface and plugs in the type of values and ID that it works with: Account and String, respectively. MongoRepository in turn extends PagingAndSortingRepository interface defined in Spring Data Commons.\n  You can define other queries by declaring their method signatures. In this case, add findByAccountOwner, which essentially seeks documents of type Account and finds the documents that match on accountOwner.\n  You also have findByAccountNumberAndRoutingNumber, which finds account by accountnumber and routingnumber.\n  Testing Using Junit5 and Spring\u0026rsquo;s TestContext framework to create MongoDB repository Integration tests\n@ExtendWith(SpringExtension.class) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT, classes = GreenfieldReactiveApplication.class) public class AccountRepositoryTest { @Autowired private AccountRepository accountRepository; @Test public void testSave() { //when  Account account = repository.save(new Account(null, 918345L, 234518L, \u0026#34;alex\u0026#34;)); //then  assertThat(account.getId()).isNotNull(); } @Test public void testFindById() { //given  Account account = repository.save(new Account(null, 918345L, 234518L, \u0026#34;alex\u0026#34;)); //when  Account createdCustomerAccount = repository.findById(account.getId()); //then  assertThat(\u0026#34;alex\u0026#34;).isEqualTo(createdCustomerAccount.getAccountOwner()); assertThat(234518L).isEqualTo(createdCustomerAccount.getRoutingNumber()); assertThat(createdCustomerAccount.getId()).isNotNull(); } @Test public void testFindByAccountNumberAndRoutingNumber() { //given  Account account = repository.save(new Account(null, 918345L, 234518L, \u0026#34;alex\u0026#34;)); //when  Account createdCustomerAccount = repository.findByAccountNumberAndRoutingNumber(account.getAccountNumber(), account.getRoutingNumber); //then  assertThat(\u0026#34;alex\u0026#34;).isEqualTo(createdCustomerAccount.getAccountOwner()); assertThat(234518L).isEqualTo(createdCustomerAccount.getRoutingNumber()); assertThat(createdCustomerAccount.getId()).isNotNull(); } }  NOTE "},{"uri":"/recipes-non-reactive/configure-oracle/","title":"3. Configure Oracle","tags":["persistence","hikari","database connection pool","anti patterns"],"description":"","content":"CONTEXT This recipe is part of a cookbook for developing a modern cloud ready microservice using the greenfield-app-starter.\nThis recipe deals with configuring Oracle to fulfill persistence needs in the microservice.\nA Hikari datasource bean with connection pooling is created after completing this recipe.\nPrerequisite  STEP 1: CREATE APPLICATION USING THE STARTER is completed.  SOLUTION   Define environment variables for oracle database connection properties, these are usually defined in uDeploy and never hardcoded inapplication.yml\n   Property Details     ORACLE_DB_URL jdbc:oracle:thin:@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;:\u0026lt;schema\u0026gt; [thin] OR jdbc:oracle:oci:@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;:\u0026lt;schema\u0026gt; [oci]   ORACLE_SCHEMA database schema name   ORACLE_USERNAME username   ORACLE_PASSWORD encrypted password      Tweak (if necessary) the default database connection pool settings after reviewing the anti-patterns.\n   Property Description Starter Default     max-pool-size maximum size that the pool is allowed to reach, includes both idle and in-use connections 10   min-idle minimum number of idle connections, for efficient performance keep this value at 50% of max-pool-size or less 5   connection-timeout the maximum number of milliseconds that a client will wait for a connection from the pool. If this time is exceeded without a connection becoming available, a SQLException will be thrown. Minimum values is 250ms. 1000   idle-timeout the maximum amount of time a connection is allowed to sit idle in the pool. This setting only applies when min-idle \u0026lt; max-pool-size. Minimum is 10000ms (10s). 10000   max-lifetime the maximum lifetime of a connection in the pool. Minimum values allowed is 30000ms (30s) 1800000    NOTE\n refrain from increasing the max-pool-size beyond 15 the connection-timeoutcan be increased by small increments for networks with higher than normal latencies.    Navigate to the \u0026lt;microservice\u0026gt; directory\n  Review the database connection and connection pool properties in src/main/resources/application.yml\napplication:id:\u0026lt;wells fargo distributed-id\u0026gt;persistence:oracle:name:oracle-datasourceurl:${oracle.db.url}username:${oracle.username}password:${oracle.password}schema:${oracle.schema}dialect:org.hibernate.dialect.Oracle12cDialectdriver:oracle.jdbc.OracleDriverconnection-pool:max-pool-size:10idle-pool-size:2connection-timeout:500idle-timeout:800max-lifetime:3000   replace \u0026lt;wells fargo distributed-id\u0026gt; with the distributed ID the placeholder property is defined as an environment variable in uDeploy.  e.g. environment variable: ORACLE_DB_URL corresponds to ${oracle.db.url} the ${placeholder} is used for automatic variable expansion during microservice startup.    Validation   Open a command window in the \u0026lt;microservice-name\u0026gt; directory\n  Set all the required environment variables for the property placeholders inapplication.yml\nset ORACLE_DB_URL=jdbc:oracle:thin:@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;:\u0026lt;schema\u0026gt; set ORACLE_SCHEMA=schema-name set ORACLE_USERNAME=username set ORACLE_PASSWORD=encrypted password   Validate the new microservice \u0026hellip;\n can be built locally: gradlew clean bootJar runs locally: java -jar build/libs/\u0026lt;app-name\u0026gt;-\u0026lt;version\u0026gt;.jar    Verify microservice health in the browser\n  http://localhost:8080/actuator/info\n  http://localhost:8080/actuator/health\nthe datasource details and uptime status should be displayed in the browser.\n  http://localhost:8080/actuator/beans\nsearch for HikariDataSource in the browser.\n    NOTES  database connection pool anti-patterns  "},{"uri":"/recipes-non-reactive/configure-mongodb/","title":"4. Configure MongoDB","tags":["persistence","mongodb","spring boot"],"description":"","content":"CONTEXT This is the second recipe in the series, for developing a modern cloud ready microservice using the greenfield-app-starter.\nThis recipe deals with configuring persistence in the microservice.\nPrerequisite  STEP 1: CREATE APPLICATION USING THE STARTER is completed.  SOLUTION   Determine and record the following mongodb database connection details\n   Property Details     uri mongodb://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database name\u0026gt;   database-name database name   username username (plain text)   password password (encrypted)      Navigate to the \u0026lt;microservice\u0026gt; directory\n  Update the database connection and connection pool requirements in src/main/resources/application.yml\napplication:name:description:id:\u0026lt;wells fargo distributed-id\u0026gt;persistence:mongodb:host:port:database-name:username:password:  Validation   Open a command window in the \u0026lt;microservice-name\u0026gt; directory\n  Validate the new microservice\n can be built locally: gradlew bootJar runs locally: gradlew bootRun    Verify microservice health in the browser\n  http://localhost:8080/actuator/info\n  http://localhost:8080/actuator/health\nthe datasource details and uptime status should be displayed in the browser.\n  http://localhost:8080/actuator/beans\nsearch for mongo in the browser and you should see the following\n\u0026#34;mongo\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;4.2.0\u0026#34; } },     NOTES you can also use properties provided by Spring to configure mongodb\nspring:mongodb:uri:database:username:password:"},{"uri":"/best-practices/git-commit-messages/","title":"Git Commit Best Practices","tags":["practices","spring boot","microservice","application development","git"],"description":"Git Commit Best Practices","content":"CONTEXT This recipe provides details of how to write good commit message.\nWhat is a commit message? The commit command is used to save changes to a local repository after staging in Git. However, before you can save changes in Git, you have to tell Git which changes you want to save. A great way to do that is by adding a commit message to identify your changes.\nWhy commit messages are important?   Commit messages provides a way to communicate with your developers on a project. The change defines how you achieved something, but the commit message explains why you are doing it. It should provide enough context to avoid a developer wonder why code is written like that.\n  Commit messages are not only for coworkers but also for our future self as Peter Hutterer explains in this article\n   Any software project is a collaborative project. It has at least two developers, the original developer and the original developer a few weeks or months later when the train of thought has long left the station.\n  Git commands like log, rebase, cherry-pick can provide great insights from commit history if the commits are meaningful. Writing a good commit messages makes it easier to review with git log -p  How to write a good commit message  Chris Beams describes these three points in his article: How to Write a Git Commit Message.   Style. Markup syntax, wrap margins, grammar, capitalization, punctuation. Spell these things out, remove the guesswork, and make it all as simple as possible. The end result will be a remarkably consistent log that’s not only a pleasure to read but that actually does get read on a regular basis.\n  Content. What kind of information should the body of the commit message (if any) contain? What should it not contain?\n  Metadata. How should issue tracking IDs, pull request numbers, etc. be referenced?\n  Here are 7 rules to write a good commit message:  Separate subject from body with a blank line. Limit the subject line to 50 characters. Capitalize the subject line. Do not end the subject line with a period. Use the imperative mood in the subject line. Wrap the body at 72 characters. Use the body to explain what and why vs. how.    "},{"uri":"/common/create-app-using-starter/","title":"1. Create barebone microservice using the starter","tags":["starter","microservice","barebone microservice"],"description":"","content":"CONTEXT This recipe is part of a cookbook for developing a modern cloud ready microservice using the greenfield-app-starter.\nUpon completion of this recipe, you\u0026rsquo;ll have a working Spring Boot microservice with enabled actuator (info,health and metrics) endpoints.\nPrerequisite  JDK 8 or higher IntelliJ or Eclipse IDE GIT client Access to greenfield-app-starter Github repo  SOLUTION   Collect or record the following details for the new microservice\n   Property Description Example     lob line of business consumer   business-group business group within the lob lending   application-group application category or grouping loan   microservice-name camelCased name AutoLoanCalculator   microservice-version in major.minor.patch format; Versioning Basics 0.1.0   description short phrase describing the purpose of the microservice consumer auto loan calculator for period less than 36 months   JDK-version Java 8 or above; one of 1.8, 1.11, 1.12, 1.13 or 1.14 1.12   project-group com.wellsfargo.\u0026lt;lob\u0026gt;.\u0026lt;business-group\u0026gt;.\u0026lt;application-group\u0026gt; com.wellsfargo.consumer.lending.loan      Clone the greenfield-app-starter from Gitlab repo git clone \u0026lt;repo url\u0026gt;   Rename folder: greenfield-app-starter to \u0026lt;microservice-name\u0026gt;\n(example: AutoLoanCalculator)\n  Update the microservice name in settings.gradle\nrootProject.name = \u0026#34;\u0026lt;microservice-name\u0026gt;\u0026#34; # EXAMPLE rootProject.name = \u0026#34;AutoLoanCalculator\u0026#34;   Update project details in build.gradle\ndescription = \u0026#34;\u0026lt;description\u0026gt;\u0026#34; group = \u0026#34;\u0026lt;project-group\u0026gt;\u0026#34; version = \u0026#34;\u0026lt;microservice-version\u0026gt;\u0026#34; sourceCompatibility = \u0026#34;\u0026lt;JDK-version\u0026gt;\u0026#34; # EXAMPLE description = \u0026#34;consumer auto loan calculator for period less than 36 months\u0026#34; group = \u0026#34;com.wellsfargo.consumer.lending.loan\u0026#34; version = \u0026#34;0.1.0\u0026#34; sourceCompatibility = \u0026#34;1.12\u0026#34;   Rename package from com.wellsfargo.cto.eai.starter to \u0026lt;project-group\u0026gt;\n(example: com.wellsfargo.consumer.lending.loan)\n  Rename main application classname from GreenfieldMicroservice to \u0026lt;microservice-name\u0026gt;\n(example: AutoLoanCalculator)\n  As an example, the barebone microservice will have the following:\n folder: AutoLoanCalculator containing  com.wellsfargo.consumer.lending.loan.AutoLoanCalculator.java src/main/resources/application.yml build.gradle settings.gradle      Create the codebase folder/package structure based on the recipe in Best Practices\n  Validation   Open a command window in the \u0026lt;microservice-name\u0026gt; directory\n  Validate the new microservice\n can be built locally: gradlew bootJar runs locally: gradlew bootRun    Verify microservice health in the browser\n http://localhost:8080/actuator/info http://localhost:8080/actuator/health    Next Step Follow the Reactive or Non-Reactive path depending on your microservice needs.\nNOTES  Why use Spring Boot ? How to version software ?  "},{"uri":"/common/","title":"Let&#39;s get started","tags":["start"],"description":"","content":"Cookbook with recipes for using the greenfield application starters to develop modern cloud native microservice in Wells Fargo.\nThe cookbook and starters were created by VMWare Tanzu (Pivotal) Labs during the WellsFargo Enterprise Architecture Consulting engagement in January 2021.\nCONTEXT The following spring boot starters in Github will help Wells Fargo developers to build non-reactive or reactive microservices based on the business need:\n greenfield-app-starter for migrating a legacy application to a cloud-ready microservice. greenfield-reactive-app-starter for greenfield applications, to take advantage of the non-blocking behavior which improves application performance and resiliency.  Both the application starters share a common Application Tech Stack, comprising of components approved for use within Wells Fargo.\nSOLUTION Build Non-Reactive microservice Complete the recipes in following order:\n Create barebone microservice using the starter Configure Actuators Configure Oracle Configure MongoDB  Build Reactive microservice Complete the recipes in following order:\n Create barebone microservice using the starter Configure Actuators Configure MongoDB  NOTES  What is Reactive Programming ? Essence of Reactive Programming  "},{"uri":"/best-practices/spring-boot-structure/","title":"Organize codebase","tags":["practices","spring boot","microservice","application development"],"description":"A simple guide to organize codebase in a microservice","content":"CONTEXT This recipe provides details for organizing codebase in a typical spring boot microservice.\nSOLUTION Layers Why even have layers today? What year is this?  Most of the examples I see have one Spring bean which does configuration, endpoints, data, security, validation and pancakes\u0026hellip;\n An argument can be made that trivial services (especially focused examples) have no need for the overhead of multiple layers, packages, separation of duties, abstraction, and even basic organization.\nAfter all, we aren\u0026rsquo;t building monoliths anymore, we\u0026rsquo;re building microsevices right?\nYes, but we are looking for a balance here between overhead and chaos.\nWe know at least the following:\n  We need just enough abstraction for things to change easily (layers \u0026amp; interfaces)\n  Other developers have to make sense of our code (convention \u0026amp; organization)\n  Our code has to be easily testable (isolation \u0026amp; separation of duties).\n  Another big one: Annotation based frameworks commonly utilize dynamic proxies which only works on public method calls to spring-injected beans (i.e. hystrix, JPA, transactions etc.)\n   But even the Spring Initializr only gives me dependencies, and a single Application class\n The rest is left to you simply because there are so many options.\nStill here? Let\u0026rsquo;s look at a possible strategy.\nAPI Based Services Most http/api Spring Boot applications consist of 3 primary layer types.\n Controller (API Front End - Integration with external consumers) | v Services (POJO Capabilities i.e. the valuable stuff) | v Integration (Integration with external producers) Integration classes are injected into services, services are injected into controllers or other services for composition.\nCalls through layers go down, never up (no integration classes calling Services).\nController Layer This is the API exposure layer. It accepts and returns http request/response and defines how external clients will interact with the service.\nYou can organize parts of the API into separate controller classes, though sometimes this is an indication that the service should be split into smaller microservices.\nThis should be the only layer that \u0026ldquo;knows\u0026rdquo; it is a webservice at all (don\u0026rsquo;t pass raw http request/response through service layer).\n IMPORTANT: Do not make database calls or calls to other services in the controller.\nAs much as possible, place only http related mapping/translating/versioning code here (and potentially authorization code depending on your design).\n It is simply a http adapter to the capabilities of your service.\nThe controller layer has the final exception handling responsibilities as well: Controller Advice\nService Layer This layer contains one or more classes representing the internal composable capabilities of the app.\nThe name \u0026ldquo;service\u0026rdquo; used here can sometimes confuse developers new to Spring vernacular. It refers to classes which are the composable, plain Java, \u0026ldquo;middle bit\u0026rdquo; of the application which do not deal with protocols or other integration concerns.\nServices should contain things like business logic, data transformation, user authorization and calls to the DAOs.\nServices can aggregate/compose/orchestrate across other services\nIntegration Layer Contains one or more classes which perform integrations with other webservices, databases, message queues, etc.\nCommon naming conventions for integration classes depending on their purpose:\n DAO (A bit too generic, but widely used) Repository (Mostly CRUD data operations) Sender (Message sender over Kafka, JMS, AMQP etc.)   UNIMPORTANT: For an integration which is more than just data operations (think tax calculation), some folks use a different suffix such as \u0026ldquo;Engine\u0026rdquo;.\n Example:\n AccountController | AccountService / \\ AccountDataService CustomerService / \\ *AccountRepo* *CustomerRepo* Integration classes should follow an interface/implementation pattern.\nAccountRepo  would be the interface, JDBCAccountRepo would be a potential implementation of the interface (I know naming things is hard, Please don\u0026rsquo;t call it an \u0026ldquo;Impl\u0026rdquo;)\nMessage Services Message services are structured nearly the same as an API service, but instead of a controller they have a message listener.\nExample:\n AccountMessageListener | AccountService | AccountRepo Non-layer Classes Models Contains plain java classes which represent data passed between the layers of an application.\nEach layer may have its own models depending on the abstraction needed (one model or \u0026ldquo;translate everywhere\u0026rdquo;).\nMany difficult trade-offs are made here between simplicity and abstraction.\nApplication A single class which declares your app as Spring Boot-able.\nConfig Any spring bean config which is not embedded in the bean itself (annotations) will go here. This is usually limited to security config and declaration of spring beans that you did not write.\nInput Validation Where to do input validation for attribute x is often a subject of debate.\nSince there is no one right answer for all input types, we\u0026rsquo;ll just list some things to consider when making the decision.\nWherever you choose to validate, it\u0026rsquo;s usually best to move this to its own class.\nConsiderations:\n Some input validation (format or required elements) can change depending on the API version of the service Input validation error responses should include the attribute value and location which caused the error (if safe to do so) Some input validation is consistent across API versions and duplication of code may not be desirable If you use Spring Bean Validation, you have less code to write but also less control over where/when things happen  API Versioning  NOTE: This section is not meant to be a full API versioning strategy (that\u0026rsquo;s a different recipe). It should only recommend where to implement a versioning strategy within the layers described above. If it accidentally oversteps, refer to the main versioning recipe.\n API major-versioning should be done as a last resort because it adds a hefty maintenance cost. Favor backward compatible API changes when possible.\nIf you choose to maintain multiple major versions of an API (within the same app) this versioning should be done in the controller. Any data translation which is version specific should be done here.\nEach major version should be represented by a separate controller class\nExample: AccountControllerV1 \u0026amp; AccountControllerV2\nTo illustrate, consider the account-service which supports 3 versions concurrently: Option #1 - Chaining Translation\n AccountControllerV1 -\u0026gt; AccountControllerV2 -\u0026gt; AccountControllerV3 | AccountService | AccountRepo Option #2 - Direct Translation\n AccountControllerV1 AccountControllerV2 AccountControllerV3 \\ | / AccountService | AccountRepo The AccountService (business logic and orchestration) does not know anything about versioning. It implicitly represents the highest version.\nThe highest version controller (AccountControllerV3) does not do any versioning, it just calls the AccountService .\nLower version controllers either translate up in a chain (v1-\u0026gt;v2-\u0026gt;v3) or translate and directly call the AccountService  (v1-\u0026gt;v3).\nDoes this mean I have a different set of API models for each controller/version? If the major version change is non-trivial and involves structural changes to the API model classes, yes. Another reason to avoid this.\nPackage Organization Again, there are many ways to do this just keep in mind that consistency across projects makes it easier for new developers (though keeping services extremely small makes this slightly less important).\nHorizontal or vertical? Package by layer or package by feature? https://dzone.com/articles/project-package-organization\nPackage by Layer Example com.wellsfargo.cto.eai/ TrackingApplication.java com.wellsfargo.cto.eai.config/ SecurityConfig.java RedisConfig.java com.wellsfargo.cto.eai.controller/ AccountController.java AccountControllerAdvice.java AccountInputValidator.java com.wellsfargo.cto.eai.controller.model/ AccountRequest.java AccountResponse.java ... com.wellsfargo.cto.eai.model/ AccountStatus.java Customer.java ... com.wellsfargo.cto.eai.service/ AccountService.java CustomerService.java com.wellsfargo.cto.eai.repo/ AccountRepo.java JDBCAcountRepo.java CustomerRepo.java CDSCustomerRepo.java com.wellsfargo.cto.eai.repo.model/ Customer.java\tPackage by Feature Example "},{"uri":"/best-practices/custom-spring-validation/","title":"Custom Spring Bean Validation","tags":["practices","spring boot","microservice","custom spring bean validation"],"description":"A guide to use custom validation","content":"Context While implementing Spring REST endpoints for Spring boot applications, adding validations (inbuilt/custom) becomes inevitable. For most cases the inbuilt validators provided by JSR 380, also known as Bean Validation 2.0 framework would suffice. Some of the inbuilt validators provided are: @NotNull, @NotEmpty, @NotBlank, @Min, @Max, @Size to name a few. There are still instances where the validation need can’t be taken care of by the inbuilt validators provided by JSR 380 and in such cases we need to write custom validators which takes care of providing custom validation logic to the bean attributes.\nUse Case: Let’s assume a use case wherein we need to validate customer location details with custom validation of fields locationId, countryCode and postCode. These three fields should accept only numeric strings (ex: “123\u0026quot;)\n@Getter @Setter public class CustomerLocation { @NumericString(message = \u0026#34;locationId should be numeric\u0026#34;) private String locationId; @NotBlank(message = \u0026#34;city cannot not be empty\u0026#34;) private String city; @NumericString(message = \u0026#34;countryCode should be numeric\u0026#34;) private String countryCode; @NumericString(message = \u0026#34;postCode should be numeric\u0026#34;) private String postCode; } In the above example both inbuilt (@NotBlank) and custom (@NumericString) validators are being used. @NotBlank would ensure that the value passed to city attribute is not blank however @NumericString validator would ensure that the value passed to locationId, countryCode \u0026amp; postCode is a numeric string\nSetup: Add below dependency to build.gradle. The latest dependency can be checked here\n compile group: 'org.hibernate', name: 'hibernate-validator', version: '4.2.0.Final' If we\u0026rsquo;re using Spring Boot, then we need below dependency to be added, which will bring in the hibernate-validator dependency also.\n implementation: 'org.springframework.boot:spring-boot-starter-validation:2.4.1' Controller: Lets see the REST endpoint which validates the incoming request:\n@RestController @Validated public class ValidatorController { @PostMapping(value=\u0026#34;/v1/validate\u0026#34;, produces = \u0026#34;application/json\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; validateCustomerLocation(@ValidCustomerLocation @RequestBody CustomerLocation customerLocation){ return new ResponseEntity\u0026lt;\u0026gt;(HttpStatus.ACCEPTED); } } Note that we have to add Spring’s @Validated annotation to the controller at class level to tell Spring to evaluate the constraint annotations on method parameters. The @Validated annotation is only evaluated on class level in this case, even though it’s allowed to be used on methods.\nNew Custom Annotation: Creating a custom validator entails us rolling out our own annotation and using it in our model to enforce the validation rules. ValidCustomerLocation is a custom validator annotation for which the constraints would be validated by CustomerLocationValidator class as shown below:\n@Target({ElementType.FIELD, ElementType.PARAMETER}) @Retention(RetentionPolicy.RUNTIME) @Constraint(validatedBy = {CustomerLocationValidator.class}) @Documented public @interface ValidCustomerLocation { String message() default \u0026#34;Invalid customer location\u0026#34;; Class\u0026lt;?\u0026gt;[] groups() default {}; Class\u0026lt;? extends Payload\u0026gt;[] payload() default {}; } The @Constraint annotation defined in the class is going to validate our field and message() is the error message that is returned to the client. The additional code is boilerplate code that conforms to Spring standards.\nCustom Validator: The validation class (CustomerLocationValidator) implements the ConstraintValidator interface and must implement the isValid() method. It\u0026rsquo;s in this method we will define our validation rules.\npublic class CustomerLocationValidator implements ConstraintValidator\u0026lt;CustomerLocationConstraint, CustomerLocation\u0026gt; { @Autowired Validator validator; @Override public boolean isValid(com.wellsfargo.cto.eai.customvalidator.model.CustomerLocation customerLocation, ConstraintValidatorContext constraintValidatorContext) { Set\u0026lt;ConstraintViolation\u0026lt;com.wellsfargo.cto.eai.customvalidator.model.CustomerLocation\u0026gt;\u0026gt; constraintViolations = validator.validate(customerLocation); if (!CollectionUtils.isEmpty(constraintViolations)) { constraintValidatorContext.disableDefaultConstraintViolation(); constraintViolations.forEach(customerLocationConstraintViolation -\u0026gt; constraintValidatorContext .buildConstraintViolationWithTemplate( customerLocationConstraintViolation.getMessageTemplate()) .addConstraintViolation()); return false; } return true; } } Attributes with @NumericString annotation would be validated by NumericStringValidator class as shown below:\n@Target({ElementType.FIELD, ElementType.PARAMETER}) @Retention(RetentionPolicy.RUNTIME) @Constraint(validatedBy = {NumericStringValidator.class}) @Documented public @interface NumericString { String message() default \u0026#34;String should be numeric\u0026#34;; Class\u0026lt;?\u0026gt;[] groups() default {}; Class\u0026lt;? extends Payload\u0026gt;[] payload() default {}; } The implementation of NumericStringValidator would override the isValid() method and check if the attributes annotated with @NumericString contains only numerals using the regular expression check:\npublic class NumericStringValidator implements ConstraintValidator\u0026lt;NumericString, String\u0026gt; { @Override public boolean isValid(String str, ConstraintValidatorContext constraintValidatorContext) { if (str.matches(\u0026#34;[0-9]+\u0026#34;)) return true; return false; } } Unit Testing: Test: Complete code base is present at the below git location:\nhttps://github.com/rohanmukesh/spring-boot-custom-validator.git\nClone the codebase, build and run the CustomvalidatorApplication class. The application runs on default port 8080. Once it is up and running perform the below two tests:\n1. Valid request: Endpoint URL: localhost:8080/v1/validate\n{ \u0026#34;locationId\u0026#34;:\u0026#34;aa\u0026#34;, \u0026#34;countryCode\u0026#34;:\u0026#34;sns\u0026#34;, \u0026#34;postCode\u0026#34;:\u0026#34;ss\u0026#34; } Response:\nStatus: HTTP response code 202 Accepted\n2. Invalid request: Endpoint URL: localhost:8080/v1/validate\n{ \u0026#34;locationId\u0026#34;:\u0026#34;locationId\u0026#34;, \u0026#34;countryCode\u0026#34;:\u0026#34;countryCode\u0026#34;, \u0026#34;postCode\u0026#34;:\u0026#34;postCode\u0026#34; } Response: Status: HTTP response code 400 Bad Request\n{ \u0026#34;errorCode\u0026#34;: \u0026#34;400 BAD_REQUEST\u0026#34;, \u0026#34;errorMessage\u0026#34;: \u0026#34;Validation Errors\u0026#34;, \u0026#34;errors\u0026#34;: [ { \u0026#34;rejectedValue\u0026#34;: { \u0026#34;locationId\u0026#34;: \u0026#34;aa\u0026#34;, \u0026#34;city\u0026#34;: null, \u0026#34;countryCode\u0026#34;: \u0026#34;sns\u0026#34;, \u0026#34;postCode\u0026#34;: \u0026#34;ss\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;postCode should be numeric\u0026#34; }, { \u0026#34;rejectedValue\u0026#34;: { \u0026#34;locationId\u0026#34;: \u0026#34;aa\u0026#34;, \u0026#34;city\u0026#34;: null, \u0026#34;countryCode\u0026#34;: \u0026#34;sns\u0026#34;, \u0026#34;postCode\u0026#34;: \u0026#34;ss\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;city cannot not be empty\u0026#34; }, { \u0026#34;rejectedValue\u0026#34;: { \u0026#34;locationId\u0026#34;: \u0026#34;aa\u0026#34;, \u0026#34;city\u0026#34;: null, \u0026#34;countryCode\u0026#34;: \u0026#34;sns\u0026#34;, \u0026#34;postCode\u0026#34;: \u0026#34;ss\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;locationId should be numeric\u0026#34; }, { \u0026#34;rejectedValue\u0026#34;: { \u0026#34;locationId\u0026#34;: \u0026#34;aa\u0026#34;, \u0026#34;city\u0026#34;: null, \u0026#34;countryCode\u0026#34;: \u0026#34;sns\u0026#34;, \u0026#34;postCode\u0026#34;: \u0026#34;ss\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;countryCode should be numeric\u0026#34; } ] } "},{"uri":"/recipes-non-reactive/","title":"Non-Reactive Recipes","tags":[],"description":"","content":"Recipes for developing microservice using traditional non-reactive techniques.\n"},{"uri":"/recipes-reactive/","title":"Reactive Recipes","tags":[],"description":"","content":"Recipes for developing microservice using Spring Reactive framework.\n"},{"uri":"/best-practices/","title":"Best Practices","tags":["practices"],"description":"","content":"CONTEXT Best practices for developing a modern cloud ready microservice using the greenfield-app-starter.\nNOTES "},{"uri":"/tags/annotated-controllers/","title":"annotated controllers","tags":[],"description":"","content":""},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags/reactive/","title":"reactive","tags":[],"description":"","content":""},{"uri":"/tags/reactor/","title":"reactor","tags":[],"description":"","content":""},{"uri":"/categories/recipes/","title":"recipes","tags":[],"description":"","content":""},{"uri":"/tags/spring/","title":"Spring","tags":[],"description":"","content":""},{"uri":"/tags/spring-webflux/","title":"spring webflux","tags":[],"description":"","content":""},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"/tags/application-development/","title":"application development","tags":[],"description":"","content":""},{"uri":"/tags/git/","title":"git","tags":[],"description":"","content":""},{"uri":"/tags/microservice/","title":"microservice","tags":[],"description":"","content":""},{"uri":"/tags/practices/","title":"practices","tags":[],"description":"","content":""},{"uri":"/tags/spring-boot/","title":"spring boot","tags":[],"description":"","content":""},{"uri":"/tags/actuator/","title":"actuator","tags":[],"description":"","content":""},{"uri":"/tags/anti-patterns/","title":"anti patterns","tags":[],"description":"","content":""},{"uri":"/tags/data-repository/","title":"data-repository","tags":[],"description":"","content":""},{"uri":"/tags/database-connection-pool/","title":"database connection pool","tags":[],"description":"","content":""},{"uri":"/tags/endpoint/","title":"endpoint","tags":[],"description":"","content":""},{"uri":"/tags/health/","title":"health","tags":[],"description":"","content":""},{"uri":"/tags/health-check/","title":"health check","tags":[],"description":"","content":""},{"uri":"/tags/hikari/","title":"hikari","tags":[],"description":"","content":""},{"uri":"/tags/mongodb/","title":"mongodb","tags":[],"description":"","content":""},{"uri":"/tags/persistence/","title":"persistence","tags":[],"description":"","content":""},{"uri":"/tags/barebone-microservice/","title":"barebone microservice","tags":[],"description":"","content":""},{"uri":"/tags/custom-spring-bean-validation/","title":"custom spring bean validation","tags":[],"description":"","content":""},{"uri":"/tags/start/","title":"start","tags":[],"description":"","content":""},{"uri":"/tags/starter/","title":"starter","tags":[],"description":"","content":""},{"uri":"/","title":"Home","tags":[],"description":"Cookbook Home Page","content":"Application Starter Cookbook Cookbook with recipes for developing modern cloud native microservice created by VMWare Tanzu Labs during the WellsFargo Enterprise Architecture Consulting engagement.\n"},{"uri":"/_header/","title":"","tags":[],"description":"","content":"  Application Starter Cookbook\n"}]